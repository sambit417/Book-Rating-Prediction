{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":63641,"databundleVersionId":6957465,"sourceType":"competition"},{"sourceId":7172486,"sourceType":"datasetVersion","datasetId":4144216},{"sourceId":7206486,"sourceType":"datasetVersion","datasetId":4169151}],"dockerImageVersionId":30587,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-12-15T17:04:54.179500Z","iopub.execute_input":"2023-12-15T17:04:54.180196Z","iopub.status.idle":"2023-12-15T17:04:54.595596Z","shell.execute_reply.started":"2023-12-15T17:04:54.180154Z","shell.execute_reply":"2023-12-15T17:04:54.594831Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/test-data/Test.csv\n/kaggle/input/lemm-data/lemm_data.csv\n/kaggle/input/book-rating-prediction/sample_submission.csv\n/kaggle/input/book-rating-prediction/Train.csv\n/kaggle/input/book-rating-prediction/Test.csv\n","output_type":"stream"}]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport nltk\nfrom sklearn.preprocessing import LabelBinarizer\nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import PorterStemmer\nfrom wordcloud import WordCloud,STOPWORDS\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.tokenize import word_tokenize,sent_tokenize\nfrom bs4 import BeautifulSoup\nimport re,string,unicodedata\nfrom keras.preprocessing import text, sequence\nfrom sklearn.metrics import classification_report,confusion_matrix,accuracy_score\nfrom sklearn.model_selection import train_test_split\nfrom string import punctuation\nimport keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense,Embedding,LSTM,Dropout,Bidirectional,GRU\nimport tensorflow as tf","metadata":{"execution":{"iopub.status.busy":"2023-12-15T17:04:54.597749Z","iopub.execute_input":"2023-12-15T17:04:54.598446Z","iopub.status.idle":"2023-12-15T17:05:11.097469Z","shell.execute_reply.started":"2023-12-15T17:04:54.598413Z","shell.execute_reply":"2023-12-15T17:05:11.096461Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"data = pd.read_csv(\"/kaggle/input/book-rating-prediction/Train.csv\")","metadata":{"trusted":true},"execution_count":3,"outputs":[{"traceback":["\u001b[0;36m  Cell \u001b[0;32mIn[3], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    \"/kaggle/input/book-rating-prediction/Train.csv\")\u001b[0m\n\u001b[0m                                                    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unmatched ')'\n"],"ename":"SyntaxError","evalue":"unmatched ')' (3090605755.py, line 1)","output_type":"error"}]},{"cell_type":"code","source":"df = pd.DataFrame(data)\ndf","metadata":{"execution":{"iopub.status.busy":"2023-12-13T17:08:40.166662Z","iopub.execute_input":"2023-12-13T17:08:40.167088Z","iopub.status.idle":"2023-12-13T17:08:40.201158Z","shell.execute_reply.started":"2023-12-13T17:08:40.167058Z","shell.execute_reply":"2023-12-13T17:08:40.200216Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"                                 user_id   book_id  \\\n0       327858f6d967ef1567459b84252ac71a   8909152   \n1       b1bb017c681370fddd19bab11f4eb22a     13152   \n2       6fb896f0ccaeb445e2c9f580bff8f65d   9533378   \n3       2edbb0dcf49ad138ef79bd6b5f4ba390     12067   \n4       cc3ce566b0313a2f02d6ab246c990bce  20443207   \n...                                  ...       ...   \n629995  c62f6649b458d3d337851f2ebe125a8a  15858248   \n629996  288dc8c9871098c8a1b680db829275b4  13188615   \n629997  dc0d067a33a72b35af9e07b1522b827e  13330943   \n629998  945f2e70af82deb0767d739cd3b8a15b   5899779   \n629999  01b4f28fa3501fc7fafb18ab36bd91c9   6673172   \n\n                               review_id  \\\n0       e8cb23191d6c27e930243a08ff826395   \n1       953dfd48b372f081b5f82ce1def753f7   \n2       48509a6f6128d4f2ca243e04a0cdc896   \n3       a09f7ff4eca0c8c2fbaacf4baf6b114f   \n4       93b0128f768ee9c1af8864f566e3a7b6   \n...                                  ...   \n629995  175e2381b339ff61b3448b6e9d2b8ee3   \n629996  124090e14f4ad3ae1f72a50d00f08494   \n629997  5b07fb855a48b475f594577f8f44f1f6   \n629998  fb23f44d931b39cb49f496d4613dbba0   \n629999  a40cdba52193351f595a3d1e9af46db4   \n\n                                              review_text  \\\n0       Really, I meant to get Landline when I checked...   \n1       Update - 01/08/2016 They are making a Maximum ...   \n2       I feel like I've read so many Urban Fantasy bo...   \n3       Reread in December 2009. \\n Simply a fantastic...   \n4       BIG ASS DNF \\n Ughhh. I'm so mad at myself for...   \n...                                                   ...   \n629995  Now look at that cover! My very first impressi...   \n629996  this book was good but I didn't like it as muc...   \n629997  4/5 stars \\n I went into this book with low ex...   \n629998  I read this concurrently with the original wor...   \n629999  Because of the shadows, [George Catlin's] pain...   \n\n                            date_added                    date_updated  \\\n0       Mon Aug 24 10:09:11 -0700 2015  Mon Aug 24 18:50:11 -0700 2015   \n1       Sun Jun 16 01:49:16 -0700 2013  Sat Oct 22 04:08:59 -0700 2016   \n2       Tue Nov 08 21:15:28 -0800 2016  Wed Dec 28 16:40:44 -0800 2016   \n3       Thu Oct 08 08:21:11 -0700 2009  Sat Jan 02 07:32:11 -0800 2010   \n4       Thu Jan 28 02:16:44 -0800 2016  Thu Jan 28 02:35:30 -0800 2016   \n...                                ...                             ...   \n629995  Mon May 19 02:04:46 -0700 2014  Fri Jun 30 17:27:16 -0700 2017   \n629996  Tue Aug 28 04:53:49 -0700 2012  Wed Dec 03 09:43:31 -0800 2014   \n629997  Mon Oct 05 17:06:59 -0700 2015  Mon Feb 20 17:59:29 -0800 2017   \n629998  Thu Apr 09 08:41:41 -0700 2009  Wed May 13 08:53:29 -0700 2009   \n629999  Tue Mar 17 16:52:24 -0700 2015  Tue Mar 22 23:30:31 -0700 2016   \n\n                               read_at                      started_at  \\\n0       Sat Aug 22 00:00:00 -0700 2015                             NaN   \n1                                  NaN                             NaN   \n2       Sun Nov 13 00:00:00 -0800 2016                             NaN   \n3       Sat Jan 02 00:00:00 -0800 2010                             NaN   \n4                                  NaN                             NaN   \n...                                ...                             ...   \n629995  Wed Jan 21 00:00:00 -0800 2015  Thu Jan 15 00:00:00 -0800 2015   \n629996  Wed Dec 03 09:43:31 -0800 2014  Sun Nov 30 00:00:00 -0800 2014   \n629997  Thu Jan 14 09:29:23 -0800 2016  Fri Jan 08 00:00:00 -0800 2016   \n629998  Sat May 09 00:00:00 -0700 2009                             NaN   \n629999  Sun Mar 13 00:00:00 -0800 2016  Fri Feb 12 00:00:00 -0800 2016   \n\n        n_votes  n_comments  rating  \n0             1           2       4  \n1             1           0       4  \n2             1           0       3  \n3             1           0       5  \n4             7           3       1  \n...         ...         ...     ...  \n629995       29           7       5  \n629996        0           0       3  \n629997        1           0       4  \n629998        0           0       2  \n629999        6           0       3  \n\n[630000 rows x 11 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>user_id</th>\n      <th>book_id</th>\n      <th>review_id</th>\n      <th>review_text</th>\n      <th>date_added</th>\n      <th>date_updated</th>\n      <th>read_at</th>\n      <th>started_at</th>\n      <th>n_votes</th>\n      <th>n_comments</th>\n      <th>rating</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>327858f6d967ef1567459b84252ac71a</td>\n      <td>8909152</td>\n      <td>e8cb23191d6c27e930243a08ff826395</td>\n      <td>Really, I meant to get Landline when I checked...</td>\n      <td>Mon Aug 24 10:09:11 -0700 2015</td>\n      <td>Mon Aug 24 18:50:11 -0700 2015</td>\n      <td>Sat Aug 22 00:00:00 -0700 2015</td>\n      <td>NaN</td>\n      <td>1</td>\n      <td>2</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>b1bb017c681370fddd19bab11f4eb22a</td>\n      <td>13152</td>\n      <td>953dfd48b372f081b5f82ce1def753f7</td>\n      <td>Update - 01/08/2016 They are making a Maximum ...</td>\n      <td>Sun Jun 16 01:49:16 -0700 2013</td>\n      <td>Sat Oct 22 04:08:59 -0700 2016</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>1</td>\n      <td>0</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>6fb896f0ccaeb445e2c9f580bff8f65d</td>\n      <td>9533378</td>\n      <td>48509a6f6128d4f2ca243e04a0cdc896</td>\n      <td>I feel like I've read so many Urban Fantasy bo...</td>\n      <td>Tue Nov 08 21:15:28 -0800 2016</td>\n      <td>Wed Dec 28 16:40:44 -0800 2016</td>\n      <td>Sun Nov 13 00:00:00 -0800 2016</td>\n      <td>NaN</td>\n      <td>1</td>\n      <td>0</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2edbb0dcf49ad138ef79bd6b5f4ba390</td>\n      <td>12067</td>\n      <td>a09f7ff4eca0c8c2fbaacf4baf6b114f</td>\n      <td>Reread in December 2009. \\n Simply a fantastic...</td>\n      <td>Thu Oct 08 08:21:11 -0700 2009</td>\n      <td>Sat Jan 02 07:32:11 -0800 2010</td>\n      <td>Sat Jan 02 00:00:00 -0800 2010</td>\n      <td>NaN</td>\n      <td>1</td>\n      <td>0</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>cc3ce566b0313a2f02d6ab246c990bce</td>\n      <td>20443207</td>\n      <td>93b0128f768ee9c1af8864f566e3a7b6</td>\n      <td>BIG ASS DNF \\n Ughhh. I'm so mad at myself for...</td>\n      <td>Thu Jan 28 02:16:44 -0800 2016</td>\n      <td>Thu Jan 28 02:35:30 -0800 2016</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>7</td>\n      <td>3</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>629995</th>\n      <td>c62f6649b458d3d337851f2ebe125a8a</td>\n      <td>15858248</td>\n      <td>175e2381b339ff61b3448b6e9d2b8ee3</td>\n      <td>Now look at that cover! My very first impressi...</td>\n      <td>Mon May 19 02:04:46 -0700 2014</td>\n      <td>Fri Jun 30 17:27:16 -0700 2017</td>\n      <td>Wed Jan 21 00:00:00 -0800 2015</td>\n      <td>Thu Jan 15 00:00:00 -0800 2015</td>\n      <td>29</td>\n      <td>7</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>629996</th>\n      <td>288dc8c9871098c8a1b680db829275b4</td>\n      <td>13188615</td>\n      <td>124090e14f4ad3ae1f72a50d00f08494</td>\n      <td>this book was good but I didn't like it as muc...</td>\n      <td>Tue Aug 28 04:53:49 -0700 2012</td>\n      <td>Wed Dec 03 09:43:31 -0800 2014</td>\n      <td>Wed Dec 03 09:43:31 -0800 2014</td>\n      <td>Sun Nov 30 00:00:00 -0800 2014</td>\n      <td>0</td>\n      <td>0</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>629997</th>\n      <td>dc0d067a33a72b35af9e07b1522b827e</td>\n      <td>13330943</td>\n      <td>5b07fb855a48b475f594577f8f44f1f6</td>\n      <td>4/5 stars \\n I went into this book with low ex...</td>\n      <td>Mon Oct 05 17:06:59 -0700 2015</td>\n      <td>Mon Feb 20 17:59:29 -0800 2017</td>\n      <td>Thu Jan 14 09:29:23 -0800 2016</td>\n      <td>Fri Jan 08 00:00:00 -0800 2016</td>\n      <td>1</td>\n      <td>0</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>629998</th>\n      <td>945f2e70af82deb0767d739cd3b8a15b</td>\n      <td>5899779</td>\n      <td>fb23f44d931b39cb49f496d4613dbba0</td>\n      <td>I read this concurrently with the original wor...</td>\n      <td>Thu Apr 09 08:41:41 -0700 2009</td>\n      <td>Wed May 13 08:53:29 -0700 2009</td>\n      <td>Sat May 09 00:00:00 -0700 2009</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>0</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>629999</th>\n      <td>01b4f28fa3501fc7fafb18ab36bd91c9</td>\n      <td>6673172</td>\n      <td>a40cdba52193351f595a3d1e9af46db4</td>\n      <td>Because of the shadows, [George Catlin's] pain...</td>\n      <td>Tue Mar 17 16:52:24 -0700 2015</td>\n      <td>Tue Mar 22 23:30:31 -0700 2016</td>\n      <td>Sun Mar 13 00:00:00 -0800 2016</td>\n      <td>Fri Feb 12 00:00:00 -0800 2016</td>\n      <td>6</td>\n      <td>0</td>\n      <td>3</td>\n    </tr>\n  </tbody>\n</table>\n<p>630000 rows × 11 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"df['rating'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2023-12-13T17:08:44.786985Z","iopub.execute_input":"2023-12-13T17:08:44.787400Z","iopub.status.idle":"2023-12-13T17:08:44.807151Z","shell.execute_reply.started":"2023-12-13T17:08:44.787374Z","shell.execute_reply":"2023-12-13T17:08:44.805847Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"rating\n4    219581\n5    185505\n3    132280\n2     50839\n0     21692\n1     20103\nName: count, dtype: int64"},"metadata":{}}]},{"cell_type":"code","source":"df['length']=df['review_text'].apply(lambda x:len(x))\nl=(df['length'])\nmax(l)","metadata":{"execution":{"iopub.status.busy":"2023-12-13T17:08:47.226905Z","iopub.execute_input":"2023-12-13T17:08:47.227289Z","iopub.status.idle":"2023-12-13T17:08:47.559494Z","shell.execute_reply.started":"2023-12-13T17:08:47.227258Z","shell.execute_reply":"2023-12-13T17:08:47.558535Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"19826"},"metadata":{}}]},{"cell_type":"code","source":"stop = set(stopwords.words('english'))\npunctuation = list(string.punctuation)\nstop.update(punctuation)","metadata":{"execution":{"iopub.status.busy":"2023-12-13T18:10:37.050075Z","iopub.execute_input":"2023-12-13T18:10:37.050405Z","iopub.status.idle":"2023-12-13T18:10:37.055935Z","shell.execute_reply.started":"2023-12-13T18:10:37.050378Z","shell.execute_reply":"2023-12-13T18:10:37.054936Z"},"trusted":true},"execution_count":45,"outputs":[]},{"cell_type":"code","source":"import nltk\nimport re\nfrom nltk.stem import PorterStemmer, WordNetLemmatizer\n# Download NLTK resources\nnltk.download('punkt')\nnltk.download('stopwords')\nnltk.download('wordnet')\ndef strip_html(text):\n    soup = BeautifulSoup(text, \"html.parser\")\n    return soup.get_text()\n\n#Removing the square brackets\ndef remove_between_square_brackets(text):\n    return re.sub('\\[[^]]*\\]', '', text)\n# Removing URL's\ndef remove_between_square_brackets(text):\n    return re.sub(r'http\\S+', '', text)\n#Removing the stopwords from text\ndef remove_stopwords(text):\n    final_text = []\n    for i in text.split():\n        if i.strip().lower() not in stop:\n            final_text.append(i.strip())\n    return \" \".join(final_text)\ndef remove_numbers(text):\n    return re.sub(r'\\d+', '', text)\n\ndef remove_punctuation(text):\n    return re.sub(r'[^\\w\\s]', '', text)\ndef tokenization(text):\n    tokens = re.split(r'\\W+', text)\n    return tokens\ndef stemming(tokens):\n    porter_stemmer = PorterStemmer()\n    return [porter_stemmer.stem(word) for word in tokens]\n\n# # Function for lemmatization\n# def lemmatization(tokens):\n#     lemmatizer = WordNetLemmatizer()\n#     return [lemmatizer.lemmatize(word) for word in tokens]\n#Removing the noisy text\ndef denoise_text(text):\n    text = text.lower() \n    text = strip_html(text)\n    text = remove_between_square_brackets(text)\n    text = remove_stopwords(text)\n    text = remove_numbers(text)\n    text = remove_punctuation(text)\n    text= tokenization(text)\n#     text = lemmatization(text)\n    return text\n\ndf['cleaned_review'] = df['review_text'].apply(denoise_text)\ndf['cleaned_review'] = df['cleaned_review'].apply(stemming)\ndf.head()\n","metadata":{"execution":{"iopub.status.busy":"2023-12-13T18:10:38.616766Z","iopub.execute_input":"2023-12-13T18:10:38.618371Z","iopub.status.idle":"2023-12-13T18:37:45.038406Z","shell.execute_reply.started":"2023-12-13T18:10:38.618342Z","shell.execute_reply":"2023-12-13T18:37:45.037760Z"},"trusted":true},"execution_count":46,"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n[nltk_data] Downloading package wordnet to /usr/share/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_47/2964451503.py:9: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n  soup = BeautifulSoup(text, \"html.parser\")\n","output_type":"stream"},{"execution_count":46,"output_type":"execute_result","data":{"text/plain":"                            user_id   book_id  \\\n0  327858f6d967ef1567459b84252ac71a   8909152   \n1  b1bb017c681370fddd19bab11f4eb22a     13152   \n2  6fb896f0ccaeb445e2c9f580bff8f65d   9533378   \n3  2edbb0dcf49ad138ef79bd6b5f4ba390     12067   \n4  cc3ce566b0313a2f02d6ab246c990bce  20443207   \n\n                          review_id  \\\n0  e8cb23191d6c27e930243a08ff826395   \n1  953dfd48b372f081b5f82ce1def753f7   \n2  48509a6f6128d4f2ca243e04a0cdc896   \n3  a09f7ff4eca0c8c2fbaacf4baf6b114f   \n4  93b0128f768ee9c1af8864f566e3a7b6   \n\n                                         review_text  \\\n0  Really, I meant to get Landline when I checked...   \n1  Update - 01/08/2016 They are making a Maximum ...   \n2  I feel like I've read so many Urban Fantasy bo...   \n3  Reread in December 2009. \\n Simply a fantastic...   \n4  BIG ASS DNF \\n Ughhh. I'm so mad at myself for...   \n\n                       date_added                    date_updated  \\\n0  Mon Aug 24 10:09:11 -0700 2015  Mon Aug 24 18:50:11 -0700 2015   \n1  Sun Jun 16 01:49:16 -0700 2013  Sat Oct 22 04:08:59 -0700 2016   \n2  Tue Nov 08 21:15:28 -0800 2016  Wed Dec 28 16:40:44 -0800 2016   \n3  Thu Oct 08 08:21:11 -0700 2009  Sat Jan 02 07:32:11 -0800 2010   \n4  Thu Jan 28 02:16:44 -0800 2016  Thu Jan 28 02:35:30 -0800 2016   \n\n                          read_at started_at  n_votes  n_comments  rating  \\\n0  Sat Aug 22 00:00:00 -0700 2015        NaN        1           2       4   \n1                             NaN        NaN        1           0       4   \n2  Sun Nov 13 00:00:00 -0800 2016        NaN        1           0       3   \n3  Sat Jan 02 00:00:00 -0800 2010        NaN        1           0       5   \n4                             NaN        NaN        7           3       1   \n\n   length                                     cleaned_review  \n0    1788  [realli, meant, get, landlin, check, one, reas...  \n1     124  [updat, make, maximum, ride, movi, and, look, ...  \n2     503  [feel, like, ive, read, mani, urban, fantasi, ...  \n3     191  [reread, decemb, simpli, fantast, read, full, ...  \n4     276  [big, ass, dnf, ughhh, im, mad, pick, even, ca...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>user_id</th>\n      <th>book_id</th>\n      <th>review_id</th>\n      <th>review_text</th>\n      <th>date_added</th>\n      <th>date_updated</th>\n      <th>read_at</th>\n      <th>started_at</th>\n      <th>n_votes</th>\n      <th>n_comments</th>\n      <th>rating</th>\n      <th>length</th>\n      <th>cleaned_review</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>327858f6d967ef1567459b84252ac71a</td>\n      <td>8909152</td>\n      <td>e8cb23191d6c27e930243a08ff826395</td>\n      <td>Really, I meant to get Landline when I checked...</td>\n      <td>Mon Aug 24 10:09:11 -0700 2015</td>\n      <td>Mon Aug 24 18:50:11 -0700 2015</td>\n      <td>Sat Aug 22 00:00:00 -0700 2015</td>\n      <td>NaN</td>\n      <td>1</td>\n      <td>2</td>\n      <td>4</td>\n      <td>1788</td>\n      <td>[realli, meant, get, landlin, check, one, reas...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>b1bb017c681370fddd19bab11f4eb22a</td>\n      <td>13152</td>\n      <td>953dfd48b372f081b5f82ce1def753f7</td>\n      <td>Update - 01/08/2016 They are making a Maximum ...</td>\n      <td>Sun Jun 16 01:49:16 -0700 2013</td>\n      <td>Sat Oct 22 04:08:59 -0700 2016</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>1</td>\n      <td>0</td>\n      <td>4</td>\n      <td>124</td>\n      <td>[updat, make, maximum, ride, movi, and, look, ...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>6fb896f0ccaeb445e2c9f580bff8f65d</td>\n      <td>9533378</td>\n      <td>48509a6f6128d4f2ca243e04a0cdc896</td>\n      <td>I feel like I've read so many Urban Fantasy bo...</td>\n      <td>Tue Nov 08 21:15:28 -0800 2016</td>\n      <td>Wed Dec 28 16:40:44 -0800 2016</td>\n      <td>Sun Nov 13 00:00:00 -0800 2016</td>\n      <td>NaN</td>\n      <td>1</td>\n      <td>0</td>\n      <td>3</td>\n      <td>503</td>\n      <td>[feel, like, ive, read, mani, urban, fantasi, ...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2edbb0dcf49ad138ef79bd6b5f4ba390</td>\n      <td>12067</td>\n      <td>a09f7ff4eca0c8c2fbaacf4baf6b114f</td>\n      <td>Reread in December 2009. \\n Simply a fantastic...</td>\n      <td>Thu Oct 08 08:21:11 -0700 2009</td>\n      <td>Sat Jan 02 07:32:11 -0800 2010</td>\n      <td>Sat Jan 02 00:00:00 -0800 2010</td>\n      <td>NaN</td>\n      <td>1</td>\n      <td>0</td>\n      <td>5</td>\n      <td>191</td>\n      <td>[reread, decemb, simpli, fantast, read, full, ...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>cc3ce566b0313a2f02d6ab246c990bce</td>\n      <td>20443207</td>\n      <td>93b0128f768ee9c1af8864f566e3a7b6</td>\n      <td>BIG ASS DNF \\n Ughhh. I'm so mad at myself for...</td>\n      <td>Thu Jan 28 02:16:44 -0800 2016</td>\n      <td>Thu Jan 28 02:35:30 -0800 2016</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>7</td>\n      <td>3</td>\n      <td>1</td>\n      <td>276</td>\n      <td>[big, ass, dnf, ughhh, im, mad, pick, even, ca...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"df['cleaned_review']","metadata":{"execution":{"iopub.status.busy":"2023-12-13T18:37:45.040211Z","iopub.execute_input":"2023-12-13T18:37:45.040968Z","iopub.status.idle":"2023-12-13T18:37:45.050701Z","shell.execute_reply.started":"2023-12-13T18:37:45.040935Z","shell.execute_reply":"2023-12-13T18:37:45.049978Z"},"trusted":true},"execution_count":47,"outputs":[{"execution_count":47,"output_type":"execute_result","data":{"text/plain":"0         [realli, meant, get, landlin, check, one, reas...\n1         [updat, make, maximum, ride, movi, and, look, ...\n2         [feel, like, ive, read, mani, urban, fantasi, ...\n3         [reread, decemb, simpli, fantast, read, full, ...\n4         [big, ass, dnf, ughhh, im, mad, pick, even, ca...\n                                ...                        \n629995    [look, cover, first, impress, first, saw, cove...\n629996                [book, good, like, much, first, book]\n629997    [, star, went, book, low, expect, night, circu...\n629998    [read, concurr, origin, work, necessari, order...\n629999    [shadow, georg, catlin, paint, direct, forc, p...\nName: cleaned_review, Length: 630000, dtype: object"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from imblearn.over_sampling import SMOTE\n# from imblearn.under_sampling import RandomUnderSampler\n# from imblearn.pipeline import Pipeline\n# from sklearn.svm import SVC\n# from sklearn.model_selection import train_test_split\n# from sklearn.feature_extraction.text import TfidfVectorizer\n# from sklearn.metrics import classification_report, accuracy_score, f1_score\n# import pandas as pd\n\n# # Assuming df is your DataFrame with 'cleaned_review' and 'rating' columns\n\n# # Step 1: Prepare the Data\n# X = df['cleaned_review']\n# y = df['rating']\n\n# # Convert class labels to strings\n# y = y.astype(str)\n\n# # Step 2: Split the Data\n# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# # Convert the lists to strings\n# X_train = [\" \".join(review) for review in X_train]\n# X_test = [\" \".join(review) for review in X_test]\n\n# # Step 3: TF-IDF Vectorization\n# tfidf_vectorizer = TfidfVectorizer()\n# X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n# X_test_tfidf = tfidf_vectorizer.transform(X_test)","metadata":{"execution":{"iopub.status.busy":"2023-12-13T18:38:57.922354Z","iopub.execute_input":"2023-12-13T18:38:57.922779Z","iopub.status.idle":"2023-12-13T18:39:45.755239Z","shell.execute_reply.started":"2023-12-13T18:38:57.922741Z","shell.execute_reply":"2023-12-13T18:39:45.754222Z"},"trusted":true},"execution_count":49,"outputs":[]},{"cell_type":"code","source":"# from imblearn.under_sampling import RandomUnderSampler\n# from imblearn.over_sampling import RandomOverSampler\n# unique_classes = df['rating'].unique()\n\n# unique_classes = df['rating'].unique()\n# sampling_strategy_undersample = {str(cls): 90000 for cls in unique_classes if str(cls) in ['3']}\n\n# undersampler = RandomUnderSampler(sampling_strategy=sampling_strategy_undersample, random_state=42)\n# X_resampled, y_resampled = undersampler.fit_resample(X_train_tfidf, y_train)\n\n# # Step 2: Oversample classes '0', '1', and '2'\n# oversampler = RandomOverSampler(sampling_strategy={'0': 50000, '1': 50000}, random_state=42)\n# X_resampled, y_resampled = oversampler.fit_resample(X_resampled, y_resampled)\n","metadata":{"execution":{"iopub.status.busy":"2023-12-13T18:48:07.395077Z","iopub.execute_input":"2023-12-13T18:48:07.395431Z","iopub.status.idle":"2023-12-13T18:48:13.758802Z","shell.execute_reply.started":"2023-12-13T18:48:07.395407Z","shell.execute_reply":"2023-12-13T18:48:13.757353Z"},"trusted":true},"execution_count":63,"outputs":[]},{"cell_type":"code","source":"# from sklearn.model_selection import train_test_split\n# from sklearn.feature_extraction.text import TfidfVectorizer\n# from sklearn.linear_model import LogisticRegression\n# from sklearn.metrics import classification_report, accuracy_score, f1_score\n# import pandas as pd\n\n# # Step 5: Build and Train the Model with Logistic Regression\n# logreg_model = LogisticRegression(max_iter=1000, multi_class='multinomial', solver='lbfgs', random_state=42)\n# logreg_model.fit(X_resampled, y_resampled)\n\n# # Step 6: Evaluate the Model\n# y_pred = logreg_model.predict(X_test_tfidf)\n\n# # Calculate accuracy\n# accuracy = accuracy_score(y_test, y_pred)\n# print(\"Accuracy:\", accuracy)\n\n# # Calculate F1 score\n# f1 = f1_score(y_test, y_pred, average='weighted')\n# print(\"F1 Score:\", f1)\n\n# # Print classification report\n# print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv(\"/kaggle/input/lemm-data/lemm_data.csv\")","metadata":{"execution":{"iopub.status.busy":"2023-12-15T17:07:06.821192Z","iopub.execute_input":"2023-12-15T17:07:06.821595Z","iopub.status.idle":"2023-12-15T17:07:33.899681Z","shell.execute_reply.started":"2023-12-15T17:07:06.821565Z","shell.execute_reply":"2023-12-15T17:07:33.898811Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"df","metadata":{"execution":{"iopub.status.busy":"2023-12-15T17:08:56.688279Z","iopub.execute_input":"2023-12-15T17:08:56.689141Z","iopub.status.idle":"2023-12-15T17:08:56.721699Z","shell.execute_reply.started":"2023-12-15T17:08:56.689103Z","shell.execute_reply":"2023-12-15T17:08:56.720527Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"                                 user_id   book_id  \\\n0       d9a0f14b836e2634b89a6a7d4d9aa184  23308084   \n1       3af7e1cda1d80d6a6d73e06eab301368  23310751   \n2       dd669721e136c1be47d739b14fa23d20    210252   \n3       aec624fd1ad0034f2553b7dc55ee1cd0    472392   \n4       d67aef0988e1814a819259eb11c92788  17675462   \n...                                  ...       ...   \n629995  34d07fb9e04bfbbb17371435223b120e    296182   \n629996  c3d7ca2c133140d684ab4d958d5e4ee9   5776788   \n629997  60982541be85a0611e9634b4f63d0cb0      3648   \n629998  b43eaf1760e1b11bc224815a3f3c48a3  13507212   \n629999  2c2822bfcdeb65ca48db551d4cfd16ef  23857998   \n\n                               review_id  \\\n0       7bde8725cbb9ceb697c72c12262dff53   \n1       a9aa8356ef1ca470c98712e17005517b   \n2       8739baeb543858142605442041d79524   \n3       1791472bb94c9733802303ecf34d9c53   \n4       d98212782db1271607a94c5836ef6189   \n...                                  ...   \n629995  3de0c7af9700e937b488e129ca3c9cc9   \n629996  96a1196bfce7fc2d4d4e589d990badc9   \n629997  a857718ca7e70b8c0ffc5ead14512fb8   \n629998  393045562fb081cf0a6975a2f6b91908   \n629999  941af42d5d9c694bbbbf8b4f8297b8c7   \n\n                                              review_text  \\\n0       My only complaint is this isn't a trilogy...I ...   \n1       i read this 2 days ago.... and remember nothin...   \n2       I wasn't a fan of the first Monster Blood, and...   \n3       Love Theo so much. The interaction between she...   \n4       a good book, just had to get through the middl...   \n...                                                   ...   \n629995  **edit 11/26/13 \\n My advice regarding the Vor...   \n629996  This is not a book I would likely have picked ...   \n629997  This book's summary boasts a spectacular story...   \n629998  Excellent sequel to Wolf Hall- covering a shor...   \n629999  Such a beautifully, complicated, different sto...   \n\n                            date_added                    date_updated  \\\n0       Thu Jul 23 22:27:34 -0700 2015  Wed May 04 20:04:40 -0700 2016   \n1       Thu Oct 13 19:33:58 -0700 2016  Thu Oct 13 19:34:28 -0700 2016   \n2       Thu Feb 27 01:44:54 -0800 2014  Sun Oct 18 20:46:53 -0700 2015   \n3       Sun Aug 28 08:19:18 -0700 2016  Sat Sep 03 17:42:00 -0700 2016   \n4       Mon Jan 11 07:20:47 -0800 2016  Mon Mar 14 07:30:27 -0700 2016   \n...                                ...                             ...   \n629995  Sat Jul 06 23:27:10 -0700 2013  Thu Jul 10 23:35:49 -0700 2014   \n629996  Tue Oct 19 17:38:21 -0700 2010  Wed Dec 22 09:46:33 -0800 2010   \n629997  Thu Jun 08 22:25:07 -0700 2017  Thu Jun 08 22:28:54 -0700 2017   \n629998  Sun Jan 08 08:38:38 -0800 2017  Sun Jan 15 06:16:15 -0800 2017   \n629999  Fri Apr 17 09:59:49 -0700 2015  Fri Apr 17 13:40:44 -0700 2015   \n\n                               read_at                      started_at  \\\n0       Wed May 04 00:00:00 -0700 2016  Mon Apr 25 00:00:00 -0700 2016   \n1       Mon Oct 10 00:00:00 -0700 2016                             NaN   \n2       Thu Feb 27 00:00:00 -0800 2014                             NaN   \n3       Fri Sep 02 03:12:59 -0700 2016  Sun Aug 28 00:00:00 -0700 2016   \n4       Sun Mar 13 00:00:00 -0800 2016  Mon Jan 11 00:00:00 -0800 2016   \n...                                ...                             ...   \n629995                             NaN                             NaN   \n629996  Tue Dec 21 00:00:00 -0800 2010  Sat Dec 18 00:00:00 -0800 2010   \n629997  Fri Jun 09 07:16:37 -0700 2017                             NaN   \n629998                             NaN                             NaN   \n629999  Fri Apr 17 14:47:26 -0700 2015  Fri Apr 17 00:00:00 -0700 2015   \n\n        n_votes  n_comments  rating  \\\n0             1           1       5   \n1             3           0       2   \n2             0           0       2   \n3             0           0       4   \n4             0           0       4   \n...         ...         ...     ...   \n629995        8           3       4   \n629996        0           0       3   \n629997       13           0       2   \n629998        0           0       5   \n629999        7           0       4   \n\n                                        preprocessed_text  \n0       ['complaint', 'be', 'nt', 'trilogyi', 'want', ...  \n1       ['read', 'day', 'ago', 'rememb', 'noth', 'disa...  \n2       ['be', 'nt', 'fan', 'first', 'monster', 'blood...  \n3       ['love', 'theo', 'much', 'interact', 'brother'...  \n4       ['good', 'book', 'get', 'middl', 'part', 'look...  \n...                                                   ...  \n629995  ['edit', 'advic', 'regard', 'vorkosigan', 'sag...  \n629996  ['book', 'would', 'like', 'pick', 'read', 'get...  \n629997  ['book', 'summari', 'boast', 'spectacular', 's...  \n629998  ['excel', 'sequel', 'wolf', 'hall', 'cover', '...  \n629999  ['beauti', 'complic', 'differ', 'stori', 'some...  \n\n[630000 rows x 12 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>user_id</th>\n      <th>book_id</th>\n      <th>review_id</th>\n      <th>review_text</th>\n      <th>date_added</th>\n      <th>date_updated</th>\n      <th>read_at</th>\n      <th>started_at</th>\n      <th>n_votes</th>\n      <th>n_comments</th>\n      <th>rating</th>\n      <th>preprocessed_text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>d9a0f14b836e2634b89a6a7d4d9aa184</td>\n      <td>23308084</td>\n      <td>7bde8725cbb9ceb697c72c12262dff53</td>\n      <td>My only complaint is this isn't a trilogy...I ...</td>\n      <td>Thu Jul 23 22:27:34 -0700 2015</td>\n      <td>Wed May 04 20:04:40 -0700 2016</td>\n      <td>Wed May 04 00:00:00 -0700 2016</td>\n      <td>Mon Apr 25 00:00:00 -0700 2016</td>\n      <td>1</td>\n      <td>1</td>\n      <td>5</td>\n      <td>['complaint', 'be', 'nt', 'trilogyi', 'want', ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>3af7e1cda1d80d6a6d73e06eab301368</td>\n      <td>23310751</td>\n      <td>a9aa8356ef1ca470c98712e17005517b</td>\n      <td>i read this 2 days ago.... and remember nothin...</td>\n      <td>Thu Oct 13 19:33:58 -0700 2016</td>\n      <td>Thu Oct 13 19:34:28 -0700 2016</td>\n      <td>Mon Oct 10 00:00:00 -0700 2016</td>\n      <td>NaN</td>\n      <td>3</td>\n      <td>0</td>\n      <td>2</td>\n      <td>['read', 'day', 'ago', 'rememb', 'noth', 'disa...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>dd669721e136c1be47d739b14fa23d20</td>\n      <td>210252</td>\n      <td>8739baeb543858142605442041d79524</td>\n      <td>I wasn't a fan of the first Monster Blood, and...</td>\n      <td>Thu Feb 27 01:44:54 -0800 2014</td>\n      <td>Sun Oct 18 20:46:53 -0700 2015</td>\n      <td>Thu Feb 27 00:00:00 -0800 2014</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>0</td>\n      <td>2</td>\n      <td>['be', 'nt', 'fan', 'first', 'monster', 'blood...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>aec624fd1ad0034f2553b7dc55ee1cd0</td>\n      <td>472392</td>\n      <td>1791472bb94c9733802303ecf34d9c53</td>\n      <td>Love Theo so much. The interaction between she...</td>\n      <td>Sun Aug 28 08:19:18 -0700 2016</td>\n      <td>Sat Sep 03 17:42:00 -0700 2016</td>\n      <td>Fri Sep 02 03:12:59 -0700 2016</td>\n      <td>Sun Aug 28 00:00:00 -0700 2016</td>\n      <td>0</td>\n      <td>0</td>\n      <td>4</td>\n      <td>['love', 'theo', 'much', 'interact', 'brother'...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>d67aef0988e1814a819259eb11c92788</td>\n      <td>17675462</td>\n      <td>d98212782db1271607a94c5836ef6189</td>\n      <td>a good book, just had to get through the middl...</td>\n      <td>Mon Jan 11 07:20:47 -0800 2016</td>\n      <td>Mon Mar 14 07:30:27 -0700 2016</td>\n      <td>Sun Mar 13 00:00:00 -0800 2016</td>\n      <td>Mon Jan 11 00:00:00 -0800 2016</td>\n      <td>0</td>\n      <td>0</td>\n      <td>4</td>\n      <td>['good', 'book', 'get', 'middl', 'part', 'look...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>629995</th>\n      <td>34d07fb9e04bfbbb17371435223b120e</td>\n      <td>296182</td>\n      <td>3de0c7af9700e937b488e129ca3c9cc9</td>\n      <td>**edit 11/26/13 \\n My advice regarding the Vor...</td>\n      <td>Sat Jul 06 23:27:10 -0700 2013</td>\n      <td>Thu Jul 10 23:35:49 -0700 2014</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>8</td>\n      <td>3</td>\n      <td>4</td>\n      <td>['edit', 'advic', 'regard', 'vorkosigan', 'sag...</td>\n    </tr>\n    <tr>\n      <th>629996</th>\n      <td>c3d7ca2c133140d684ab4d958d5e4ee9</td>\n      <td>5776788</td>\n      <td>96a1196bfce7fc2d4d4e589d990badc9</td>\n      <td>This is not a book I would likely have picked ...</td>\n      <td>Tue Oct 19 17:38:21 -0700 2010</td>\n      <td>Wed Dec 22 09:46:33 -0800 2010</td>\n      <td>Tue Dec 21 00:00:00 -0800 2010</td>\n      <td>Sat Dec 18 00:00:00 -0800 2010</td>\n      <td>0</td>\n      <td>0</td>\n      <td>3</td>\n      <td>['book', 'would', 'like', 'pick', 'read', 'get...</td>\n    </tr>\n    <tr>\n      <th>629997</th>\n      <td>60982541be85a0611e9634b4f63d0cb0</td>\n      <td>3648</td>\n      <td>a857718ca7e70b8c0ffc5ead14512fb8</td>\n      <td>This book's summary boasts a spectacular story...</td>\n      <td>Thu Jun 08 22:25:07 -0700 2017</td>\n      <td>Thu Jun 08 22:28:54 -0700 2017</td>\n      <td>Fri Jun 09 07:16:37 -0700 2017</td>\n      <td>NaN</td>\n      <td>13</td>\n      <td>0</td>\n      <td>2</td>\n      <td>['book', 'summari', 'boast', 'spectacular', 's...</td>\n    </tr>\n    <tr>\n      <th>629998</th>\n      <td>b43eaf1760e1b11bc224815a3f3c48a3</td>\n      <td>13507212</td>\n      <td>393045562fb081cf0a6975a2f6b91908</td>\n      <td>Excellent sequel to Wolf Hall- covering a shor...</td>\n      <td>Sun Jan 08 08:38:38 -0800 2017</td>\n      <td>Sun Jan 15 06:16:15 -0800 2017</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>0</td>\n      <td>5</td>\n      <td>['excel', 'sequel', 'wolf', 'hall', 'cover', '...</td>\n    </tr>\n    <tr>\n      <th>629999</th>\n      <td>2c2822bfcdeb65ca48db551d4cfd16ef</td>\n      <td>23857998</td>\n      <td>941af42d5d9c694bbbbf8b4f8297b8c7</td>\n      <td>Such a beautifully, complicated, different sto...</td>\n      <td>Fri Apr 17 09:59:49 -0700 2015</td>\n      <td>Fri Apr 17 13:40:44 -0700 2015</td>\n      <td>Fri Apr 17 14:47:26 -0700 2015</td>\n      <td>Fri Apr 17 00:00:00 -0700 2015</td>\n      <td>7</td>\n      <td>0</td>\n      <td>4</td>\n      <td>['beauti', 'complic', 'differ', 'stori', 'some...</td>\n    </tr>\n  </tbody>\n</table>\n<p>630000 rows × 12 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import classification_report, accuracy_score, f1_score\nimport pandas as pd\n\n# Assuming df is your DataFrame with 'cleaned_review' and 'rating' columns\n\n# Step 1: Prepare the Data\nX = df['preprocessed_text']\ny = df['rating']\n\n# Step 2: Split the Data\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n\n\n# Step 3: TF-IDF Vectorization\ntfidf_vectorizer = TfidfVectorizer()\nX_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\nX_test_tfidf = tfidf_vectorizer.transform(X_test)\n\n# Step 4: Build and Train the Model\nlogreg_model = LogisticRegression(max_iter=500, multi_class='multinomial', solver='sag', random_state=42)\nlogreg_model.fit(X_train_tfidf, y_train)\n\n# Step 5: Evaluate the Model\ny_pred = logreg_model.predict(X_test_tfidf)\n\n# Calculate accuracy\naccuracy = accuracy_score(y_test, y_pred)\nprint(\"Accuracy:\", accuracy)\n\n# Calculate F1 score\nf1 = f1_score(y_test, y_pred, average='weighted')\nprint(\"F1 Score:\", f1)\n\n# Print classification report\nprint(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n\n\n","metadata":{"execution":{"iopub.status.busy":"2023-12-15T17:50:23.717838Z","iopub.execute_input":"2023-12-15T17:50:23.718203Z","iopub.status.idle":"2023-12-15T17:52:23.400740Z","shell.execute_reply.started":"2023-12-15T17:50:23.718174Z","shell.execute_reply":"2023-12-15T17:52:23.399449Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"Accuracy: 0.5192539682539683\nF1 Score: 0.5120267474497875\nClassification Report:\n               precision    recall  f1-score   support\n\n           0       0.52      0.25      0.34      4280\n           1       0.47      0.28      0.35      3997\n           2       0.42      0.27      0.33     10289\n           3       0.45      0.41      0.43     26306\n           4       0.49      0.60      0.54     43846\n           5       0.62      0.62      0.62     37282\n\n    accuracy                           0.52    126000\n   macro avg       0.50      0.41      0.44    126000\nweighted avg       0.52      0.52      0.51    126000\n\n","output_type":"stream"}]},{"cell_type":"code","source":"df[[\"rating\"]]","metadata":{"execution":{"iopub.status.busy":"2023-12-12T01:53:40.023518Z","iopub.status.idle":"2023-12-12T01:53:40.024433Z","shell.execute_reply.started":"2023-12-12T01:53:40.024123Z","shell.execute_reply":"2023-12-12T01:53:40.024145Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data1 = pd.read_csv(\"/kaggle/input/book-rating-prediction/Test.csv\")","metadata":{"execution":{"iopub.status.busy":"2023-12-13T18:59:36.166794Z","iopub.status.idle":"2023-12-13T18:59:36.167107Z","shell.execute_reply.started":"2023-12-13T18:59:36.166947Z","shell.execute_reply":"2023-12-13T18:59:36.166961Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df1 = pd.DataFrame(data1)\ndf1","metadata":{"execution":{"iopub.status.busy":"2023-12-13T18:59:36.168254Z","iopub.status.idle":"2023-12-13T18:59:36.168561Z","shell.execute_reply.started":"2023-12-13T18:59:36.168400Z","shell.execute_reply":"2023-12-13T18:59:36.168415Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import nltk\nimport re\nfrom nltk.stem import PorterStemmer, WordNetLemmatizer\n# Download NLTK resources\nnltk.download('punkt')\nnltk.download('stopwords')\nnltk.download('wordnet')\ndef strip_html(text):\n    soup = BeautifulSoup(text, \"html.parser\")\n    return soup.get_text()\n\n#Removing the square brackets\ndef remove_between_square_brackets(text):\n    return re.sub('\\[[^]]*\\]', '', text)\n# Removing URL's\ndef remove_between_square_brackets(text):\n    return re.sub(r'http\\S+', '', text)\n#Removing the stopwords from text\ndef remove_stopwords(text):\n    final_text = []\n    for i in text.split():\n        if i.strip().lower() not in stop:\n            final_text.append(i.strip())\n    return \" \".join(final_text)\ndef remove_numbers(text):\n    return re.sub(r'\\d+', '', text)\n\ndef remove_punctuation(text):\n    return re.sub(r'[^\\w\\s]', '', text)\ndef tokenization(text):\n    tokens = re.split(r'\\W+', text)\n    return tokens\ndef stemming(tokens):\n    porter_stemmer = PorterStemmer()\n    return [porter_stemmer.stem(word) for word in tokens]\n\n# # Function for lemmatization\n# def lemmatization(tokens):\n#     lemmatizer = WordNetLemmatizer()\n#     return [lemmatizer.lemmatize(word) for word in tokens]\n#Removing the noisy text\ndef denoise_text(text):\n    text = text.lower() \n    text = strip_html(text)\n    text = remove_between_square_brackets(text)\n    text = remove_stopwords(text)\n    text = remove_numbers(text)\n    text = remove_punctuation(text)\n    text= tokenization(text)\n#     text = lemmatization(text)\n    return text\n\ndf1['cleaned_review'] = df1['review_text'].apply(denoise_text)\n# df1['cleaned_review'] = df1['cleaned_review'].apply(stemming)\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df1.head()","metadata":{"execution":{"iopub.status.busy":"2023-12-13T18:59:36.171729Z","iopub.status.idle":"2023-12-13T18:59:36.172577Z","shell.execute_reply.started":"2023-12-13T18:59:36.172357Z","shell.execute_reply":"2023-12-13T18:59:36.172380Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nX_test_new = df1['cleaned_review']\n\n\nX_test_new = [\" \".join(review) for review in X_test_new]\n\n\nX_test_tfidf_new = tfidf_vectorizer.transform(X_test_new)\n\n\ny_pred_new = logreg_model.predict(X_test_tfidf_new)\n\n\ndf1['rating'] = y_pred_new\n\n\nprint(df1[['cleaned_review', 'rating']])\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission =df1[[\"review_id\",\"rating\"]]\nsubmission\n","metadata":{"execution":{"iopub.status.busy":"2023-12-13T05:05:13.929407Z","iopub.execute_input":"2023-12-13T05:05:13.930275Z","iopub.status.idle":"2023-12-13T05:05:13.956885Z","shell.execute_reply.started":"2023-12-13T05:05:13.930238Z","shell.execute_reply":"2023-12-13T05:05:13.955518Z"},"trusted":true},"execution_count":43,"outputs":[{"execution_count":43,"output_type":"execute_result","data":{"text/plain":"                               review_id rating\n0       606d3b7ba5cc90e4069b1e225b84deea      5\n1       70388d316638176b827ea060839971ef      4\n2       840a4fb63336a3fc2213e5016cafbca2      3\n3       6c5865dc54856d1dcb317c3dd42215a8      5\n4       0fdf6c933c0b299ebf5053b8d9a2f950      4\n...                                  ...    ...\n269995  f69f612c8a460fc5d13ea414d370f636      4\n269996  78e5e4d5427adab67653bcb5331f1da0      5\n269997  1b7dfa5240b9b9412567c98f9c592eae      5\n269998  587ca6fc22ce2ca7b7cffca98e6a4b4e      0\n269999  364ee34248a66910cd9435188e93e204      5\n\n[270000 rows x 2 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>review_id</th>\n      <th>rating</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>606d3b7ba5cc90e4069b1e225b84deea</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>70388d316638176b827ea060839971ef</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>840a4fb63336a3fc2213e5016cafbca2</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>6c5865dc54856d1dcb317c3dd42215a8</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0fdf6c933c0b299ebf5053b8d9a2f950</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>269995</th>\n      <td>f69f612c8a460fc5d13ea414d370f636</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>269996</th>\n      <td>78e5e4d5427adab67653bcb5331f1da0</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>269997</th>\n      <td>1b7dfa5240b9b9412567c98f9c592eae</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>269998</th>\n      <td>587ca6fc22ce2ca7b7cffca98e6a4b4e</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>269999</th>\n      <td>364ee34248a66910cd9435188e93e204</td>\n      <td>5</td>\n    </tr>\n  </tbody>\n</table>\n<p>270000 rows × 2 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"submission=submission.to_csv(\"submission07.csv\",index=None)\npd.read_csv(\"submission07.csv\")","metadata":{"execution":{"iopub.status.busy":"2023-12-13T18:59:36.177059Z","iopub.status.idle":"2023-12-13T18:59:36.177547Z","shell.execute_reply.started":"2023-12-13T18:59:36.177327Z","shell.execute_reply":"2023-12-13T18:59:36.177343Z"},"trusted":true},"execution_count":null,"outputs":[]}]}